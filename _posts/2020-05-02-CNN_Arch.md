---
title: 'CNN Architectures'
permalink: /posts/cnn/
[//]: #tags:
[//]: #  - CNN
[//]: #  - Architectures
---

In this article, I am going to summarize the key-ideas of the following CNN architectures:
  1. AlexNet
  2. VGG
  3. Inception
  4. Rethinking Inception
  5. ResNet
  6. Wide ResNet
  7. ResNext
  8. DenseNet
  9. ShuffleNet
  10. MobileNet-V2
 
## 1. AlexNet
<p align="center">
  <img src="https://github.com/sportsunrahul/sportsunrahul.github.io/blob/master/images/cnn/AlexNet.PNG?raw=true" alt="Photo" style="width: 450px;"/> 
</p>
Idea: *ReLU, Dropout, GPU, ImageNet*

This was one of the first network which started giving good result on ImageNet. Main changes incroporated in AlexNet that are sorted according to their importance accoding to authors are:

**ReLU Non-Linearity:** Earlier models used to face this issue of saturating neurons that resulted in vanishing gradients problem. Rectified Linear Unit, which is just max(0, x), solved this problem. ReLU led to faster convergence of the network.
    
**Training on Multiple GPUs:** Since we want to train bigger network, and at the time of this paper, there were not GPUs which could support this big network, authors went for multiple GPU training. Kerenls are split into two GPUs with one trick: the GPUs communicate only in certain layers. 

**Local Response Normalization:** Even though ReLU does not have the problem of saturating neurons authors found improvement in generalization after applying input normalization. Activity of neruons is normalized over a set of adjacent kernel maps. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. This is applied in certain layers of the network.

**Overlapping Pooling:** Authors find that overlapping pooling generally find it difficult to overlap. Though I am not sure about this.

Filter size were of varying sizes which went on becoming smaller with layers: 11x11, 5x5, 3x3.

## 2. VGG
<p align="center">
  <img src="https://github.com/sportsunrahul/sportsunrahul.github.io/blob/master/images/cnn/vgg.png?raw=true" alt="Photo" style="width: 450px;"/> 
</p>
Idea: *Smaller filter*
Authors were able to train a very deep network which was a challenge at that time. They used several techniques to do that. 

**Smaller filters:** Suppose we want to get a receptive field of 7x7. One way to get that is using 7x7 filter which has 49 parameters and 7*7=49 operations to caluclate one activation. Other thing we can do is stack three 3x3 filters. This will have 3x3x3 = 27 parameters and 9+9+9=27 operation to calculate one activaion. This is the key idea of this paper. We can get big receptive field by using smaller filters. This has added advantage of incorporation of three non-linearity instead of single one which makes the decision function more discriminative.

**1x1 Filters:** Though this idea mainly comes from paper *Network in Network, M. Lin, 2014*, this paper mentions it and hence I put it here. Incorporation of 1x1 helps increase non-linearity without changing the receptive.
Using two convolution back to back without non-linearity is pointless because two 3x3 is equivalent to 5x5 which is computationally expensive.

**Removed LRN:** Local response normalization is not used as it does not help in perfomance and only increases number of parameters.

VGG converges faster than AlexNet in spite of having more parameters due to possibly 2 reasons: (a) implicit regualarization due to greater depth and samller filters, (b) pre-initialization of certain layers.

*Note: 4096 parameters in the last hidden layer has no particular reason. Earlier it started due to limitation of GPU size, and later researchers blindly used it without questioning it much.*

## 3. Inception
<p align="center">
  <img src="https://github.com/sportsunrahul/sportsunrahul.github.io/blob/master/images/cnn/inception.png?raw=true" alt="Photo" style="width: 450px;"/> 
</p>

Idea: *Split, Transform, Aggregate*.

Main hallmark is improved utilization of computing resources inside the network.
Use 1x1 filter for two purposes:
	1. To increase non-linearity (aka discriminative power) without changing receptive field
	2. Dimension reduction to remove computation bottlenecks
Increasing size of network helps. But computationally expensive and prone to overfitting.
So, move from fully connected to sparsely connected architectures even inside the convolutions.
Correlated pixels will be together in lower layers (close to input). Also, there will be clusters which will be slightly far which might provide extra context. So, we need both 3x3 and 5x5.
Using them is expensive so we use 1x1 to reduce the computation.
Bring to lower dimension using 1x1 and then apply 3x3 and 5x5.
Using inception modules only in higher layers (away from input) because of memory efficiency.
This aligns with the intuition that visual information should be processed at various scales and then aggregated so that the next stage can abstract features from different scales simultaneously.




























