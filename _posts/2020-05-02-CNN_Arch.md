---
title: 'CNN Architectures'
permalink: /posts/cnn/
[//]: #tags:
[//]: #  - CNN
[//]: #  - Architectures
---

In this article, I am going to summarize the key-ideas of the following CNN architectures:
  1. AlexNet
  2. VGG
  3. Inception
  4. Rethinking Inception
  5. ResNet
  6. Wide ResNet
  7. ResNext
  8. DenseNet
  9. ShuffleNet
  10. MobileNet-V2
 
## 1. AlexNet
This was one of the first network which started giving good result on ImageNet. Main changes incroporated in AlexNet that are sorted according to their importance accoding to authors are:

**ReLU Non-Linearity:** Earlier models used to face this issue of saturating neurons that resulted in vanishing gradients problem. Rectified Linear Unit, which is just max(0, x), solved this problem. ReLU led to faster convergence of the network.
    
**Training on Multiple GPUs:** Since we want to train bigger network, and at the time of this paper, there were not GPUs which could support this big network, authors went for multiple GPU training. Kerenls are split into two GPUs with one trick: the GPUs communicate only in certain layers. 

**Local Response Normalization:** Even though ReLU does not have the problem of saturating neurons authors found improvement in generalization after applying input normalization. Activity of neruons is normalized over a set of adjacent kernel maps. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. This is applied in certain layers of the network.

**Overlapping Pooling:** Authors find that overlapping pooling generally find it difficult to overlap. Though I am not sure about this.
![AlexNet](https://github.com/sportsunrahul/sportsunrahul.github.io/blob/master/images/cnn/AlexNet.PNG)



