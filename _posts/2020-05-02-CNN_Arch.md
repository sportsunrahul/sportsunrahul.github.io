---
title: 'CNN Architectures'
permalink: /posts/cnn/
[//]: #tags:
[//]: #  - CNN
[//]: #  - Architectures
---

In this article, I am going to summarize the key-ideas of the following CNN architectures:
  1. AlexNet
  2. VGG
  3. Inception
  4. Rethinking Inception
  5. ResNet
  6. Wide ResNet
  7. ResNext
  8. DenseNet
  9. ShuffleNet
  10. MobileNet-V2
 
## 1. AlexNet
![AlexNet](https://github.com/sportsunrahul/sportsunrahul.github.io/blob/master/images/cnn/AlexNet.PNG)
<p align="center">
  <img src="https://github.com/sportsunrahul/sportsunrahul.github.io/blob/master/images/cnn/AlexNet.PNG?raw=true" alt="Photo" style="width: 450px;"/> 
</p>
This was one of the first network which started giving good result on ImageNet. Main changes incroporated in AlexNet that are sorted according to their importance accoding to authors are:

**ReLU Non-Linearity:** Earlier models used to face this issue of saturating neurons that resulted in vanishing gradients problem. Rectified Linear Unit, which is just max(0, x), solved this problem. ReLU led to faster convergence of the network.
    
**Training on Multiple GPUs:** Since we want to train bigger network, and at the time of this paper, there were not GPUs which could support this big network, authors went for multiple GPU training. Kerenls are split into two GPUs with one trick: the GPUs communicate only in certain layers. 

**Local Response Normalization:** Even though ReLU does not have the problem of saturating neurons authors found improvement in generalization after applying input normalization. Activity of neruons is normalized over a set of adjacent kernel maps. This sort of response normalization implements a form of lateral inhibition inspired by the type found in real neurons, creating competition for big activities amongst neuron outputs computed using different kernels. This is applied in certain layers of the network.

**Overlapping Pooling:** Authors find that overlapping pooling generally find it difficult to overlap. Though I am not sure about this.

## 2. VGG

3x3 filters.
Stack of two 3x3 is equivalent to one 5x5.
5x5 is expensive because it has 25 operations.
Two 3x3 has just 9 + 9 = 18 operations.
Using two convolution back to back without non-linearity is pointless because two 3x3 is equivalent to 5x5 which is computationally expensive.
Filter size is usually odd because you donâ€™t know where to put the answers after computing convolution.
Incorporation of 1x1 helps increase non-linearity without changing the receptive.
If you have a fully convolutional network, no need to sample multiple crops at test time.
Three 1x1 is worse than 3x3. This indicates that while additional non-linearity helps, it is also important to capture spatial context by using conv filters with non-trivial receptive fields.
Deep net with small filters (like 3x3) outperforms a shallow net with large filters (like 7x7, 11x11).



